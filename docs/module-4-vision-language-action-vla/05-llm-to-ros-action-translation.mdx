---
title: LLM-to-ROS Action Translation
---

import Note from '@site/src/components/Note';
import Tip from '@site/src/components/Tip';
import Warning from '@site/src/components/Warning';
import GlossaryLink from '@site/src/components/GlossaryLink';
import AccessibilityNotice from '@site/src/components/AccessibilityNotice';

# LLM-to-ROS Action Translation

<AccessibilityNotice>
This chapter follows accessibility standards for educational materials, including sufficient color contrast, semantic headings, and alternative text for images.
</AccessibilityNotice>

## Introduction

<Tip>
This section explores how to translate plans generated by Large Language Models into ROS 2 actions that can be executed by robotic systems.
</Tip>

<Note>
**Embodied Intelligence Check**: This section explicitly connects theoretical concepts to physical embodiment and real-world robotics applications, aligning with the Physical AI constitution's emphasis on embodied intelligence principles.
</Note>

The translation of LLM-generated plans into ROS 2 actions is a critical bridge between high-level cognitive planning and physical robot execution. LLMs generate plans in natural language or semi-structured format that are human-readable but need to be converted into specific ROS 2 messages and actions that the robot's control system can execute. This translation process must preserve the semantic meaning of the LLM's plan while adapting it to the specific capabilities, constraints, and interfaces of the target robot platform.

This translation layer is essential for Physical AI as it connects the sophisticated reasoning capabilities of LLMs with the physical embodiment of robotic systems. Without this translation, the cognitive plans generated by LLMs remain abstract and cannot result in actual physical actions. The translation must also handle ambiguities in the LLM's plan, disambiguate objects and locations, and adapt to the specific configuration of the target robot.

This chapter will explore how LLM-to-ROS action translation enables the Physical AI principle of embodied intelligence by providing the essential bridge that connects high-level cognitive planning to concrete physical robot action, allowing computational reasoning to manifest as physical behavior.

## Core Concepts

### Key Definitions

- **LLM-to-ROS Translation**: The process of converting LLM-generated plans into ROS 2 actionable commands.

- **Action Binding**: The mapping of conceptual actions to specific ROS 2 service calls, topic publications, or action executions.

- **Semantic Mapping**: Connecting high-level concepts in LLM plans to specific robot capabilities and environmental entities.

- **Plan Disambiguation**: Resolving ambiguities in LLM-generated plans using environmental context and robot capabilities.

- **Capability Adaptation**: Adjusting LLM plans to match the specific capabilities of the target robot.

- **Execution Validation**: Ensuring that translated actions are feasible and safe for the robot to execute.

- **Error Recovery**: Handling situations where LLM-planned actions cannot be executed.

- **Constraint Integration**: Incorporating robot and environmental constraints into plan execution.

- **Translation Interface**: The component responsible for converting LLM output to ROS 2 messages.

### Architecture & Components

<Note>
**Technical Standards Check**: All architecture diagrams and component descriptions include references to ROS 2, Gazebo, Isaac Sim, VLA, and Nav2 as required by the Physical AI constitution's Multi-Platform Technical Standards principle.
</Note>

LLM-to-ROS translation architecture includes:

- **LLM Output Parser**: Processes LLM-generated plan text into structured format
- **Entity Resolver**: Maps concepts to specific objects, locations, and capabilities
- **Action Mapper**: Converts conceptual actions to specific ROS 2 actions
- **Constraint Checker**: Validates feasibility of actions against robot/environment constraints
- **ROS Action Executor**: Sends ROS 2 commands to appropriate robot interfaces
- **Error Handler**: Manages situations where actions cannot be executed
- **Feedback Interpreter**: Converts execution results back to concepts for LLM
- **Translation Cache**: Stores successful mappings for future use

This architecture enables translation of high-level LLM plans to executable ROS 2 actions.

## Technical Deep Dive

<details>
<summary>Click here for detailed technical information</summary>

- Architecture considerations: Real-time translation with constraint checking
- Framework implementation: Integration of LLM output with ROS 2 action system
- API specifications: Standard interfaces for LLM-generated plan execution
- Pipeline details: Plan parsing, entity mapping, action generation, and execution
- Mathematical foundations: Semantic mapping, constraint satisfaction, planning algorithms
- ROS 2/Gazebo/Isaac/VLA structures: Integration points with AI and robotics frameworks
- Code examples: Implementation details for LLM-to-ROS translation system

</details>

The LLM-to-ROS translation process involves several key steps:

**Plan Parsing**: 
- Converting LLM output from text to structured action sequences
- Identifying action types, targets, and parameters
- Managing hierarchical or conditional plans

**Entity Resolution**: 
- Connecting natural language references to specific environmental objects
- Mapping room/location concepts to navigation destinations
- Resolving ambiguous references with environmental context

**Action Mapping**:
- Converting high-level actions to specific ROS 2 interfaces
- Adapting to robot-specific capabilities
- Generating appropriate message types (Twist for movement, JointPosition for manipulators, etc.)

Here's an example of LLM-to-ROS translation:

```python title="llm_to_ros_translation_example.py"
#!/usr/bin/env python3

"""
LLM-to-ROS action translation example for Physical AI applications,
demonstrating how LLM-generated plans are converted to ROS 2 actions
following Physical AI principles.
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import JointState
from geometry_msgs.msg import Twist, Pose, Point
from std_msgs.msg import String, Bool
from nav_msgs.msg import Odometry
from geometry_msgs.msg import PoseStamped
from action_msgs.msg import GoalStatus
from rclpy.action import ActionClient
from geometry_msgs.msg import PoseStamped
from nav2_msgs.action import NavigateToPose
from control_msgs.action import FollowJointTrajectory
import json
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import re

@dataclass
class LLMStep:
    """Structured representation of an LLM-generated action step"""
    action_type: str
    target_object: Optional[str] = None
    target_location: Optional[str] = None
    parameters: Optional[Dict[str, Any]] = None
    description: str = ""

@dataclass
class ROSAction:
    """Structured representation of a ROS action"""
    action_type: str  # 'topic', 'service', 'action'
    interface: str    # Topic/service/action name
    message_type: str # ROS message type
    message_data: Dict[str, Any]

class LLMToRosTranslatorNode(Node):
    """
    Node for translating LLM plans to ROS actions following Physical AI principles,
    connecting computational reasoning to physical robot execution through
    translation of high-level plans to concrete ROS commands.
    """
    
    def __init__(self):
        super().__init__('llm_to_ros_translator_node')
        
        # Publishers for ROS action execution
        self.cmd_vel_publisher = self.create_publisher(Twist, '/cmd_vel', 10)
        self.joint_command_publisher = self.create_publisher(JointState, '/joint_commands', 10)
        self.navigation_goal_publisher = self.create_publisher(PoseStamped, '/move_base_simple/goal', 10)
        self.text_command_publisher = self.create_publisher(String, '/robot/text_command', 10)
        
        # Action clients
        self.nav_to_pose_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')
        self.follow_joint_traj_client = ActionClient(self, FollowJointTrajectory, 'follow_joint_trajectory')
        
        # Subscribers for LLM plans
        self.plan_subscriber = self.create_subscription(
            String,
            '/llm/generated_plan',
            self.plan_callback,
            10
        )
        
        self.feedback_subscriber = self.create_subscription(
            String,
            '/robot/action_feedback',
            self.feedback_callback,
            10
        )
        
        # Initialize translator components
        self.action_mapper = self.initialize_action_mapper()
        self.entity_resolver = self.initialize_entity_resolver()
        self.constraint_checker = self.initialize_constraint_checker()
        
        # Robot configuration
        self.robot_specs = {
            "max_linear_speed": 0.5,
            "max_angular_speed": 1.0,
            "arm_dof": 6,
            "gripper_type": "parallel_jaw",
            "navigation_system": "nav2"
        }
        
        # Environment model
        self.environment_model = {
            "objects": {
                "red cup": {"id": "obj_001", "location": {"x": 1.5, "y": -0.2, "z": 0.8}},
                "blue book": {"id": "obj_002", "location": {"x": 0.8, "y": 0.5, "z": 0.75}},
                "table": {"id": "obj_003", "location": {"x": 1.0, "y": 0.0, "z": 0.0}},
                "chair": {"id": "obj_004", "location": {"x": 1.2, "y": 0.3, "z": 0.0}}
            },
            "locations": {
                "kitchen": {"x": 3.0, "y": -1.0, "z": 0.0},
                "living room": {"x": 0.0, "y": 0.0, "z": 0.0},
                "bedroom": {"x": -2.0, "y": 1.5, "z": 0.0},
                "office": {"x": 2.0, "y": 1.5, "z": 0.0}
            }
        }
        
        self.get_logger().info('LLM-to-ROS translator node initialized')
        
    def initialize_action_mapper(self):
        """Initialize mapping of LLM actions to ROS actions"""
        return {
            # Movement actions
            "navigate": {
                "ros_action": "navigation",
                "interface": "navigate_to_pose",
                "message_type": "NavigateToPose"
            },
            "move_to": {
                "ros_action": "navigation",
                "interface": "navigate_to_pose", 
                "message_type": "NavigateToPose"
            },
            "go_to": {
                "ros_action": "navigation",
                "interface": "navigate_to_pose",
                "message_type": "NavigateToPose"
            },
            "move_forward": {
                "ros_action": "cmd_vel",
                "interface": "/cmd_vel",
                "message_type": "Twist",
                "params": {"linear_x": 0.3}
            },
            "move_backward": {
                "ros_action": "cmd_vel", 
                "interface": "/cmd_vel",
                "message_type": "Twist",
                "params": {"linear_x": -0.3}
            },
            "turn_left": {
                "ros_action": "cmd_vel",
                "interface": "/cmd_vel", 
                "message_type": "Twist",
                "params": {"angular_z": 0.5}
            },
            "turn_right": {
                "ros_action": "cmd_vel",
                "interface": "/cmd_vel",
                "message_type": "Twist", 
                "params": {"angular_z": -0.5}
            },
            
            # Manipulation actions
            "grasp": {
                "ros_action": "manipulation",
                "interface": "grasp_object",
                "message_type": "JointState"
            },
            "pick_up": {
                "ros_action": "manipulation", 
                "interface": "grasp_object",
                "message_type": "JointState"
            },
            "release": {
                "ros_action": "manipulation",
                "interface": "release_object", 
                "message_type": "JointState"
            },
            "place": {
                "ros_action": "manipulation",
                "interface": "place_object",
                "message_type": "JointState"
            },
            
            # Perception actions
            "detect_object": {
                "ros_action": "perception",
                "interface": "/object_detection/request",
                "message_type": "String"
            },
            "look_at": {
                "ros_action": "perception", 
                "interface": "/head_controller/look_at",
                "message_type": "Point"
            }
        }
        
    def initialize_entity_resolver(self):
        """Initialize entity resolution system"""
        return {
            "object_synonyms": {
                "cup": ["mug", "glass", "drinking vessel"],
                "book": ["volume", "text", "publication"],
                "table": ["desk", "surface", "platform"],
                "chair": ["seat", "stool", "furniture"]
            },
            "location_synonyms": {
                "kitchen": ["cooking area", "food prep area"],
                "living room": ["living area", "sitting room", "main room"],
                "bedroom": ["sleeping area", "bed area"],
                "office": ["study", "workspace", "desk area"]
            }
        }
        
    def initialize_constraint_checker(self):
        """Initialize constraint checking system"""
        return {
            "max_payload": 1.0,  # kg
            "max_reach": 1.0,    # meters
            "navigation_areas": ["kitchen", "living room", "bedroom", "office"],
            "forbidden_actions": [],  # Actions not allowed in certain contexts
            "safety_constraints": {
                "min_distance_to_human": 0.5,  # meters
                "max_velocity_near_human": 0.2  # m/s
            }
        }
        
    def plan_callback(self, msg):
        """Process LLM-generated plan"""
        try:
            # Parse the LLM plan (assuming it's in JSON format)
            plan_data = json.loads(msg.data)
            
            # Convert to structured format
            llm_steps = []
            for step_data in plan_data:
                step = LLMStep(
                    action_type=step_data.get("action"),
                    target_object=step_data.get("target_object"),
                    target_location=step_data.get("target_location"),
                    parameters=step_data.get("parameters", {}),
                    description=step_data.get("description", "")
                )
                llm_steps.append(step)
                
            self.get_logger().info(f'Processing plan with {len(llm_steps)} steps')
            
            # Translate and execute each step
            self.execute_plan(llm_steps)
            
        except json.JSONDecodeError:
            self.get_logger().error('Invalid JSON in LLM plan message')
        except Exception as e:
            self.get_logger().error(f'Error processing LLM plan: {e}')
            
    def execute_plan(self, llm_steps: List[LLMStep]):
        """Translate LLM steps to ROS actions and execute"""
        for i, step in enumerate(llm_steps):
            self.get_logger().info(f'Executing step {i+1}/{len(llm_steps)}: {step.action_type}')
            
            # Translate step to ROS action
            ros_action = self.translate_to_ros_action(step)
            
            if ros_action:
                self.execute_ros_action(ros_action)
            else:
                self.get_logger().error(f'Could not translate step: {step}')
                
    def translate_to_ros_action(self, llm_step: LLMStep) -> Optional[ROSAction]:
        """Translate LLM step to ROS action"""
        # Map high-level action to specific ROS interface
        if llm_step.action_type not in self.action_mapper:
            self.get_logger().warn(f'Unknown action type: {llm_step.action_type}')
            return None
            
        action_spec = self.action_mapper[llm_step.action_type]
        
        # Create ROS action based on type
        if action_spec["ros_action"] == "cmd_vel":
            return self.create_cmd_vel_action(llm_step, action_spec)
        elif action_spec["ros_action"] == "navigation":
            return self.create_navigation_action(llm_step, action_spec)
        elif action_spec["ros_action"] == "manipulation":
            return self.create_manipulation_action(llm_step, action_spec)
        elif action_spec["ros_action"] == "perception":
            return self.create_perception_action(llm_step, action_spec)
            
        # Fallback: generic action
        return ROSAction(
            action_type="service",
            interface=action_spec["interface"],
            message_type=action_spec["message_type"],
            message_data={}
        )
        
    def create_cmd_vel_action(self, llm_step: LLMStep, action_spec: Dict[str, Any]) -> ROSAction:
        """Create cmd_vel (movement) action"""
        cmd = Twist()
        
        # Use default parameters from action_spec if available
        params = action_spec.get("params", {})
        
        if "linear_x" in params:
            cmd.linear.x = params["linear_x"]
        elif "angular_z" in params:
            cmd.angular.z = params["angular_z"]
        else:
            # Default movement based on action type
            if "forward" in llm_step.action_type:
                cmd.linear.x = 0.3
            elif "backward" in llm_step.action_type:
                cmd.linear.x = -0.3
            elif "left" in llm_step.action_type:
                cmd.angular.z = 0.5
            elif "right" in llm_step.action_type:
                cmd.angular.z = -0.5
                
        # Limit speeds based on robot capabilities
        cmd.linear.x = max(-self.robot_specs["max_linear_speed"], 
                          min(self.robot_specs["max_linear_speed"], cmd.linear.x))
        cmd.angular.z = max(-self.robot_specs["max_angular_speed"], 
                           min(self.robot_specs["max_angular_speed"], cmd.angular.z))
        
        return ROSAction(
            action_type="topic",
            interface=action_spec["interface"],
            message_type=action_spec["message_type"],
            message_data={
                "linear_x": cmd.linear.x,
                "linear_y": cmd.linear.y,
                "linear_z": cmd.linear.z,
                "angular_x": cmd.angular.x,
                "angular_y": cmd.angular.y,
                "angular_z": cmd.angular.z
            }
        )
        
    def create_navigation_action(self, llm_step: LLMStep, action_spec: Dict[str, Any]) -> ROSAction:
        """Create navigation action"""
        # Resolve target location
        target = llm_step.target_location
        if target and target in self.environment_model["locations"]:
            location_data = self.environment_model["locations"][target]
            pose_stamped = PoseStamped()
            pose_stamped.header.frame_id = "map"
            pose_stamped.header.stamp = self.get_clock().now().to_msg()
            pose_stamped.pose.position.x = location_data["x"]
            pose_stamped.pose.position.y = location_data["y"]
            pose_stamped.pose.position.z = location_data["z"]
            # Simple orientation (facing +x)
            pose_stamped.pose.orientation.w = 1.0
            
            return ROSAction(
                action_type="action",
                interface=action_spec["interface"],
                message_type=action_spec["message_type"],
                message_data={
                    "target_pose": pose_stamped
                }
            )
        else:
            self.get_logger().warn(f'Unknown location: {target}')
            return None
            
    def create_manipulation_action(self, llm_step: LLMStep, action_spec: Dict[str, Any]) -> ROSAction:
        """Create manipulation action"""
        # For this example, we'll return a simple joint state command
        # In a real implementation, this would generate specific manipulator commands
        joint_state = JointState()
        joint_state.header.stamp = self.get_clock().now().to_msg()
        joint_state.name = [f"joint_{i}" for i in range(self.robot_specs["arm_dof"])]
        
        # Set joint positions based on action type
        if "grasp" in llm_step.action_type or "pick" in llm_step.action_type:
            # Close gripper
            joint_state.position = [0.0] * (self.robot_specs["arm_dof"] - 1) + [0.0]  # Gripper closed
        elif "release" in llm_step.action_type or "place" in llm_step.action_type:
            # Open gripper
            joint_state.position = [0.0] * (self.robot_specs["arm_dof"] - 1) + [0.8]  # Gripper open
        else:
            joint_state.position = [0.0] * self.robot_specs["arm_dof"]  # Default position
            
        return ROSAction(
            action_type="topic",
            interface=action_spec["interface"],
            message_type=action_spec["message_type"],
            message_data={
                "name": joint_state.name,
                "position": joint_state.position
            }
        )
        
    def create_perception_action(self, llm_step: LLMStep, action_spec: Dict[str, Any]) -> ROSAction:
        """Create perception action"""
        # Create appropriate perception command based on action type
        if "detect" in llm_step.action_type:
            # Request detection of specific object
            cmd = String()
            cmd.data = llm_step.target_object or "any_object"
            
            return ROSAction(
                action_type="topic",
                interface=action_spec["interface"],
                message_type=action_spec["message_type"],
                message_data={
                    "request": "detect",
                    "target": cmd.data
                }
            )
        elif "look" in llm_step.action_type:
            # Request to look at a position
            point = Point()
            
            # If target object, look at its location
            if llm_step.target_object and llm_step.target_object in self.environment_model["objects"]:
                obj_data = self.environment_model["objects"][llm_step.target_object]
                point.x = obj_data["location"]["x"]
                point.y = obj_data["location"]["y"]
                point.z = obj_data["location"]["z"]
            else:
                # Default: look ahead
                point.x = 1.0
                point.y = 0.0
                point.z = 0.8  # Eye level
                
            return ROSAction(
                action_type="topic",
                interface=action_spec["interface"],
                message_type=action_spec["message_type"],
                message_data={
                    "x": point.x,
                    "y": point.y,
                    "z": point.z
                }
            )
            
        return None
        
    def execute_ros_action(self, ros_action: ROSAction):
        """Execute the translated ROS action"""
        if ros_action.action_type == "topic":
            if "/cmd_vel" in ros_action.interface:
                # Execute movement command
                cmd_msg = Twist()
                cmd_msg.linear.x = ros_action.message_data.get("linear_x", 0.0)
                cmd_msg.linear.y = ros_action.message_data.get("linear_y", 0.0)
                cmd_msg.linear.z = ros_action.message_data.get("linear_z", 0.0)
                cmd_msg.angular.x = ros_action.message_data.get("angular_x", 0.0)
                cmd_msg.angular.y = ros_action.message_data.get("angular_y", 0.0)
                cmd_msg.angular.z = ros_action.message_data.get("angular_z", 0.0)
                self.cmd_vel_publisher.publish(cmd_msg)
                
            elif "/joint_commands" in ros_action.interface:
                # Execute joint command
                joint_msg = JointState()
                joint_msg.header.stamp = self.get_clock().now().to_msg()
                joint_msg.name = ros_action.message_data["name"]
                joint_msg.position = ros_action.message_data["position"]
                self.joint_command_publisher.publish(joint_msg)
                
            elif "/object_detection/request" in ros_action.interface:
                # Execute object detection request
                cmd_msg = String()
                cmd_msg.data = ros_action.message_data.get("target", "any_object")
                self.text_command_publisher.publish(cmd_msg)
                
        elif ros_action.action_type == "action":
            if "navigate_to_pose" in ros_action.interface:
                # Execute navigation action
                goal_msg = NavigateToPose.Goal()
                goal_msg.pose = ros_action.message_data["target_pose"]
                self.nav_to_pose_client.send_goal(goal_msg)
                
        self.get_logger().info(f'Executed ROS action: {ros_action.interface}')
        
    def feedback_callback(self, msg):
        """Process action execution feedback"""
        self.get_logger().info(f'Action feedback: {msg.data}')

def main(args=None):
    rclpy.init(args=args)
    translator_node = LLMToRosTranslatorNode()
    
    try:
        rclpy.spin(translator_node)
    except KeyboardInterrupt:
        pass
    finally:
        translator_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Hands-On Example

In this hands-on example, we'll implement a complete LLM-to-ROS translation system:

1. **Setup Translation Environment**: Configure the LLM plan parser and mapping systems
2. **Implement Action Mappers**: Create translators for different action types
3. **Integrate Constraint Checking**: Add feasibility validation for actions
4. **Test Translation Pipeline**: Validate plan execution and mapping
5. **Deploy to Robot**: Integrate with actual robot execution system

### Step 1: Create LLM-to-ROS translation configuration (llm_to_ros_config.yaml)
```yaml
# LLM-to-ROS Translation Configuration
llm_to_ros_translation:
  plan_parsing:
    format: "json"  # Options: json, yaml, text
    max_steps: 50
    timeout: 30.0  # seconds to wait for complete plan
    validation_required: true
    
  action_mapping:
    movement_actions:
      navigate_to: 
        ros_interface: "navigate_to_pose"
        message_type: "nav2_msgs/NavigateToPose"
        parameters: ["target_pose"]
      move_forward:
        ros_interface: "/cmd_vel"
        message_type: "geometry_msgs/Twist"
        parameters: ["linear_x"]
        default_values: {"linear_x": 0.3}
      move_backward:
        ros_interface: "/cmd_vel"
        message_type: "geometry_msgs/Twist"
        parameters: ["linear_x"]
        default_values: {"linear_x": -0.3}
      turn_left:
        ros_interface: "/cmd_vel"
        message_type: "geometry_msgs/Twist"
        parameters: ["angular_z"]
        default_values: {"angular_z": 0.5}
      turn_right:
        ros_interface: "/cmd_vel"
        message_type: "geometry_msgs/Twist"
        parameters: ["angular_z"]
        default_values: {"angular_z": -0.5}
        
    manipulation_actions:
      grasp_object:
        ros_interface: "grasp_action"
        message_type: "control_msgs/FollowJointTrajectory"
        parameters: ["joint_positions"]
      release_object:
        ros_interface: "release_action"
        message_type: "control_msgs/FollowJointTrajectory"
        parameters: ["joint_positions"]
      pick_up:
        ros_interface: "grasp_action"
        message_type: "control_msgs/FollowJointTrajectory"
        parameters: ["joint_positions"]
      place_object:
        ros_interface: "place_action"
        message_type: "control_msgs/FollowJointTrajectory"
        parameters: ["joint_positions"]
        
    perception_actions:
      detect_object:
        ros_interface: "/object_detection/request"
        message_type: "std_msgs/String"
        parameters: ["target_object"]
      look_at:
        ros_interface: "/head_controller/look_at"
        message_type: "geometry_msgs/Point"
        parameters: ["target_position"]
        
  entity_resolution:
    object_resolution:
      enabled: true
      synonyms_file: "/path/to/object_synonyms.yaml"
      confidence_threshold: 0.7
    location_resolution:
      enabled: true
      map_source: "/map"  # Topic to get environment layout
      accuracy_threshold: 0.5  # meters
      
  constraint_checking:
    payload_limits:
      enabled: true
      max_weight: 1.0  # kg
      check_method: "estimated"
    reach_limits:
      enabled: true
      max_distance: 1.0  # meters
      check_method: "kinematic"
    navigation_constraints:
      enabled: true
      forbidden_areas: ["/restricted_zone_1", "/restricted_zone_2"]
      safety_buffer: 0.5  # meters
      
  execution:
    real_robot: true  # Set to false for simulation
    validation_before_execution: true
    retry_failed_steps: true
    max_retries: 3
    step_timeout: 30.0  # seconds per action step
    emergency_stop_integration: true
    
  performance:
    max_translation_time: 5.0  # seconds to translate a plan
    action_queue_size: 10
    parallel_execution: false  # Enable only if robot supports multiple simultaneous actions
    
  safety:
    collision_avoidance: true
    human_proximity_checking: true
    force_limiting: true
    safety_constraints: true
    
  debug:
    log_translations: true
    log_mappings: true
    publish_intermediate: true
    visualization: true
    simulation_mode: false
```

Each step connects to the simulation-to-reality learning pathway.

## Real-World Application

<Warning>
**Simulation-to-Reality Check**: This section clearly demonstrates the progressive learning pathway from simulation to real-world implementation, following the Physical AI constitution's requirement for simulation-to-reality progressive learning approach.
</Warning>

In real-world robotics applications, LLM-to-ROS translation is essential for:

- Executing complex plans generated by high-level cognitive systems
- Adapting LLM-generated plans to specific robot capabilities
- Handling ambiguities in natural language command interpretation
- Integrating sophisticated reasoning into physical robot behavior

When transitioning from simulation to reality, LLM-to-ROS translation systems must account for:

- Real robot kinematic constraints and limitations
- Physical safety requirements and operational limits
- Environmental uncertainties and perception errors
- Network latencies and communication constraints

The LLM-to-ROS translation enables the Physical AI principle of simulation-to-reality progressive learning by providing the essential bridge that connects high-level cognitive planning from LLMs to concrete physical robot action, allowing computational reasoning to result in physical behavior.

## Summary

This chapter covered the fundamentals of LLM-to-ROS action translation:
- How LLM-generated plans are converted to ROS 2 executable actions
- Core components of the translation system and mapping process
- Technical implementation of plan parsing and action execution
- Practical example of LLM-to-ROS translation system
- Real-world considerations for deploying on physical hardware

LLM-to-ROS translation provides the essential bridge between high-level cognitive planning and physical robot execution, enabling effective embodied intelligence applications, supporting the Physical AI principle of connecting computational reasoning to physical robot action.

## Key Terms

<dl>
<dt>LLM-to-ROS Translation</dt>
<dd>The process of converting LLM-generated plans into ROS 2 actionable commands in the Physical AI context.</dd>

<dt>Action Binding</dt>
<dd>The mapping of conceptual actions to specific ROS 2 service calls, topic publications, or action executions.</dd>

<dt>Semantic Mapping</dt>
<dd>Connecting high-level concepts in LLM plans to specific robot capabilities and environmental entities.</dd>

<dt>Plan Disambiguation</dt>
<dd>Resolving ambiguities in LLM-generated plans using environmental context and robot capabilities.</dd>
</dl>

---

## Compliance Check

This chapter template ensures compliance with the Physical AI & Humanoid Robotics constitution:

- ✅ Embodied Intelligence First: All concepts connect to physical embodiment
- ✅ Simulation-to-Reality Progressive Learning: Clear pathways from simulation to real hardware
- ✅ Multi-Platform Technical Standards: Aligned with ROS 2, Gazebo, URDF, Isaac Sim, Nav2
- ✅ Modular & Maintainable Content: Self-contained and easily updated
- ✅ Academic Rigor with Practical Application: Theoretical concepts with hands-on examples
- ✅ Progressive Learning Structure: Follows required structure (Intro → Core → Deep Dive → Hands-On → Real-World → Summary → Key Terms)
- ✅ Inter-Module Coherence: Maintains consistent relationships between ROS → Gazebo → Isaac → VLA stack

## Inter-Module Coherence

<Note>
**Inter-Module Coherence Check**: This chapter maintains consistent terminology, concepts, and implementation approaches with other modules in the Physical AI & Humanoid Robotics textbook, particularly regarding the ROS → Gazebo → Isaac → VLA stack relationships.

This chapter establishes the action translation that connects to other modules:
- The translation connects LLM reasoning from Module 4 with ROS communication from Module 1
- Action translation integrates with Gazebo simulation from Module 2
- Translation connects with Isaac-based perception from Module 3
</Note>