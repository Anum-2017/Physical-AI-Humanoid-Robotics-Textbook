---
title: OpenAI Whisper for Voice Commands
---

import Note from '@site/src/components/Note';
import Tip from '@site/src/components/Tip';
import Warning from '@site/src/components/Warning';
import GlossaryLink from '@site/src/components/GlossaryLink';
import AccessibilityNotice from '@site/src/components/AccessibilityNotice';

# OpenAI Whisper for Voice Commands

<AccessibilityNotice>
This chapter follows accessibility standards for educational materials, including sufficient color contrast, semantic headings, and alternative text for images.
</AccessibilityNotice>

## Introduction

<Tip>
This section explores how OpenAI's Whisper speech recognition model can be integrated into robotic systems to enable voice command processing for embodied AI applications.
</Tip>

<Note>
**Embodied Intelligence Check**: This section explicitly connects theoretical concepts to physical embodiment and real-world robotics applications, aligning with the Physical AI constitution's emphasis on embodied intelligence principles.
</Note>

OpenAI Whisper is a state-of-the-art automatic speech recognition (ASR) model that can convert human speech into text with high accuracy across multiple languages and accents. For robotic systems, Whisper provides a powerful tool for enabling natural voice interaction, allowing humans to communicate with robots using spoken language. This capability is fundamental to Physical AI as it enables robots to understand and respond to human commands in their natural form, connecting computational language understanding to physical robot behavior.

Whisper's robustness to noise, accents, and speaking styles makes it particularly suitable for real-world robotic applications where the acoustic environment may be challenging. The model can operate in real-time or near real-time when properly optimized, making it suitable for interactive robotic systems that need to respond promptly to voice commands.

This chapter will explore how Whisper enables the Physical AI principle of embodied intelligence by providing robots with the capability to understand spoken language commands, connecting computational speech recognition to physical robot embodiment and action execution.

## Core Concepts

### Key Definitions

- **Automatic Speech Recognition (ASR)**: The technology that converts spoken language into text.

- **OpenAI Whisper**: A transformer-based speech recognition model developed by OpenAI for converting speech to text.

- **Voice Command Processing**: The complete pipeline from speech input to robot action execution.

- **Speech-to-Text Pipeline**: The sequence of processing steps from audio input to textual output.

- **Acoustic Model**: A component of ASR systems that maps audio signals to phonetic units.

- **Language Model**: A component of ASR systems that provides linguistic context for word recognition.

- **Phoneme**: The smallest unit of sound that can distinguish one word from another in a language.

- **Real-time ASR**: Speech recognition that processes audio input with minimal latency.

- **Voice User Interface (VUI)**: The interface that allows humans to interact with robots using spoken language.

### Architecture & Components

<Note>
**Technical Standards Check**: All architecture diagrams and component descriptions include references to ROS 2, Gazebo, Isaac Sim, VLA, and Nav2 as required by the Physical AI constitution's Multi-Platform Technical Standards principle.
</Note>

Whisper-based voice command system includes:

- **Audio Input System**: Microphones and audio capture for voice commands
- **Preprocessing Pipeline**: Audio normalization, noise reduction, and segmentation
- **Whisper Model**: The ASR model for speech-to-text conversion
- **Post-processing Module**: Text cleaning and intent extraction
- **Command Parser**: Natural language understanding for robot commands
- **Response Generation**: Text-to-speech for robot responses
- **Audio Output System**: Speakers for robot voice responses
- **Error Handling**: Management of recognition failures and clarifications

This architecture enables robust voice command processing for embodied robotics.

## Technical Deep Dive

<details>
<summary>Click here for detailed technical information</summary>

- Architecture considerations: Real-time audio processing with ASR model integration
- Framework implementation: Integration of Whisper with ROS 2 communication
- API specifications: Standard interfaces for audio input/output and speech recognition
- Pipeline details: Audio capture, preprocessing, transcription, and command conversion
- Mathematical foundations: Transformer architectures, attention mechanisms
- ROS 2/Gazebo/Isaac/VLA structures: Integration points with AI and robotics frameworks
- Code examples: Implementation details for Whisper-based voice command systems

</details>

Whisper is built on a transformer architecture that includes both an encoder for processing audio and a decoder for generating text. The model is trained on a large corpus of audio-text pairs and is robust to various accents, background noise, and speaking styles. For robotic applications, Whisper can be used in different ways:

**Offline Processing**: Process complete audio segments for high accuracy
**Streaming Processing**: Process audio in real-time for interactive applications
**Multi-language Support**: Recognize speech in multiple languages

The Whisper model requires audio input in a specific format (typically 16kHz sampled, single-channel audio). The audio is converted to a log-mel spectrogram representation, which is then processed by the transformer model to generate text.

Here's an example of implementing Whisper for voice commands:

```python title="whisper_voice_commands_example.py"
#!/usr/bin/env python3

"""
OpenAI Whisper integration for Physical AI voice command processing,
demonstrating how speech recognition connects to robotic action following
Physical AI principles.
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import AudioData
from std_msgs.msg import String, Bool
from geometry_msgs.msg import Twist
import numpy as np
import torch
import whisper
import librosa
import threading
import queue
import time
from typing import Optional, List
import io
from pydub import AudioSegment

class WhisperVoiceCommandNode(Node):
    """
    Node for processing voice commands using OpenAI Whisper following Physical AI principles,
    connecting computational speech recognition to physical robot embodiment.
    """
    
    def __init__(self):
        super().__init__('whisper_voice_command_node')
        
        # Publishers for command processing and robot control
        self.command_publisher = self.create_publisher(String, '/robot/command', 10)
        self.control_publisher = self.create_publisher(Twist, '/cmd_vel', 10)
        self.response_publisher = self.create_publisher(String, '/robot/response', 10)
        
        # Subscribers for audio input
        self.audio_subscriber = self.create_subscription(
            AudioData,
            '/audio_input',
            self.audio_callback,
            10
        )
        
        # Initialize Whisper model
        self.model = self.load_whisper_model()
        
        # Initialize audio processing components
        self.audio_queue = queue.Queue()
        self.processing_thread = threading.Thread(target=self.process_audio_queue, daemon=True)
        self.processing_thread.start()
        
        # Voice command vocabulary for robotics
        self.robot_commands = {
            "move forward": "forward",
            "go forward": "forward", 
            "move back": "backward",
            "go back": "backward",
            "turn left": "turn_left",
            "turn right": "turn_right",
            "stop": "stop",
            "pick up": "grasp",
            "pick up the object": "grasp",
            "pick up red cup": "grasp_red_cup",
            "put down": "release",
            "turn around": "rotate_180",
            "come here": "come_to_robot",
            "go to kitchen": "navigate_kitchen",
            "go to living room": "navigate_living_room"
        }
        
        self.get_logger().info('Whisper voice command node initialized')
        
    def load_whisper_model(self):
        """Load Whisper model for speech recognition"""
        try:
            # Load the model - in a real implementation you might want to use a smaller model
            # for real-time applications, e.g., "tiny" or "base" instead of "medium"
            model = whisper.load_model("tiny.en")  # Using English-only tiny model for efficiency
            self.get_logger().info('Whisper model loaded successfully')
            return model
        except Exception as e:
            self.get_logger().error(f'Failed to load Whisper model: {e}')
            return None
            
    def audio_callback(self, msg):
        """Process incoming audio data from microphone"""
        if self.model is None:
            return
            
        # Add audio data to processing queue
        self.audio_queue.put(msg.data)
        
    def process_audio_queue(self):
        """Process audio data from queue in separate thread"""
        audio_buffer = b""
        buffer_duration_threshold = 2.0  # seconds of audio to process at once
        sample_rate = 16000  # Whisper expects 16kHz audio
        
        while rclpy.ok():
            try:
                # Get audio data from queue
                audio_chunk = self.audio_queue.get(timeout=1.0)
                audio_buffer += audio_chunk
                
                # Check if we have enough audio to process
                expected_samples = buffer_duration_threshold * sample_rate * 2  # 16-bit samples
                
                if len(audio_buffer) >= expected_samples:
                    # Process accumulated audio
                    self.process_audio_segment(audio_buffer)
                    audio_buffer = b""  # Clear buffer after processing
                else:
                    # Continue accumulating
                    continue
            except queue.Empty:
                # Process any remaining audio in buffer if it's been a while
                if len(audio_buffer) > 0:
                    # Check if buffer has at least some minimum amount of audio
                    if len(audio_buffer) > sample_rate * 2:  # At least 2 seconds of audio
                        self.process_audio_segment(audio_buffer)
                        audio_buffer = b""
                continue
            except Exception as e:
                self.get_logger().error(f'Error in audio processing thread: {e}')
                
    def process_audio_segment(self, audio_data):
        """Process a segment of audio using Whisper"""
        try:
            # Convert raw audio data to numpy array
            # Assuming 16-bit signed integers (common for AudioData messages)
            audio_int16 = np.frombuffer(audio_data, dtype=np.int16)
            
            # Normalize to [-1, 1]
            audio_float32 = audio_int16.astype(np.float32) / 32768.0
            
            # Resample to 16kHz if needed (Whisper expects 16kHz)
            # For this example, assume input is already 16kHz
        
            # Run Whisper transcription
            if self.model is not None:
                result = self.model.transcribe(audio_float32, language='en')
                recognized_text = result['text'].strip()
                
                if recognized_text:
                    self.get_logger().info(f'Recognized: "{recognized_text}"')
                    
                    # Process the recognized command
                    self.process_recognized_command(recognized_text)
                    
        except Exception as e:
            self.get_logger().error(f'Error processing audio segment: {e}')
            
    def process_recognized_command(self, recognized_text: str):
        """Process the recognized text as a robot command"""
        # Normalize the recognized text
        normalized_text = recognized_text.lower().strip()
        
        # Find matching command
        matched_command = self.match_command(normalized_text)
        
        if matched_command:
            self.get_logger().info(f'Matched command: {matched_command}')
            
            # Convert command to robot action
            robot_action = self.command_to_action(matched_command)
            
            if robot_action:
                # Publish command
                cmd_msg = String()
                cmd_msg.data = robot_action
                self.command_publisher.publish(cmd_msg)
                
                # Execute the action
                self.execute_action(robot_action)
                
                # Publish response
                response = f"OK, I will {matched_command}"
                response_msg = String()
                response_msg.data = response
                self.response_publisher.publish(response_msg)
        else:
            # Command not recognized
            self.get_logger().info(f'Command not recognized: {normalized_text}')
            
            # Publish unrecognized command response
            response_msg = String()
            response_msg.data = f"Sorry, I didn't understand '{normalized_text}'. Please repeat."
            self.response_publisher.publish(response_msg)
            
    def match_command(self, text: str) -> Optional[str]:
        """Match recognized text to known robot commands"""
        # Simple fuzzy matching strategy
        best_match = None
        best_score = 0
        
        for command_phrase, command_name in self.robot_commands.items():
            score = self.calculate_similarity(text, command_phrase)
            if score > best_score and score > 0.7:  # Threshold for similarity
                best_score = score
                best_match = command_name
                
        return best_match
        
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """Calculate similarity between two texts (simplified version)"""
        # Simple word overlap approach
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        if len(union) == 0:
            return 0.0
            
        return len(intersection) / len(union)
        
    def command_to_action(self, command: str) -> Optional[str]:
        """Convert command name to robot action string"""
        command_actions = {
            "forward": "move_forward",
            "backward": "move_backward", 
            "turn_left": "turn_left",
            "turn_right": "turn_right",
            "stop": "stop_robot",
            "grasp": "grasp_object",
            "grasp_red_cup": "grasp_red_cup",
            "release": "release_object",
            "rotate_180": "rotate_half_circle",
            "come_to_robot": "navigate_to_human",
            "navigate_kitchen": "navigate_to_kitchen",
            "navigate_living_room": "navigate_to_living_room"
        }
        
        return command_actions.get(command)
        
    def execute_action(self, action: str):
        """Execute the robot action"""
        cmd = Twist()
        
        if action == "move_forward":
            cmd.linear.x = 0.3  # Move forward at 0.3 m/s
        elif action == "move_backward":
            cmd.linear.x = -0.3  # Move backward at 0.3 m/s
        elif action == "turn_left":
            cmd.angular.z = 0.5  # Turn left at 0.5 rad/s
        elif action == "turn_right":
            cmd.angular.z = -0.5  # Turn right at 0.5 rad/s
        elif action == "stop_robot":
            cmd.linear.x = 0.0
            cmd.angular.z = 0.0
        elif action == "rotate_half_circle":
            cmd.angular.z = 0.5  # Will need to be timed for 180-degree turn
        # Other actions would be handled by higher-level systems
        
        self.control_publisher.publish(cmd)
        self.get_logger().info(f'Executed action: {action}')

def main(args=None):
    rclpy.init(args=args)
    whisper_node = WhisperVoiceCommandNode()
    
    try:
        rclpy.spin(whisper_node)
    except KeyboardInterrupt:
        pass
    finally:
        whisper_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

```python title="whisper_integration_example.py"
#!/usr/bin/env python3

"""
Example of Whisper integration with ROS 2 for voice command processing.
This example demonstrates how to use Whisper for speech-to-text in a 
robotic system following Physical AI principles.
"""

import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from sensor_msgs.msg import AudioData
import numpy as np
import torch
import whisper
from collections import deque
import threading
import time

class WhisperIntegrationNode(Node):
    """
    Integration node for Whisper ASR in robotic systems,
    following Physical AI principles for embodied intelligence through
    speech recognition that connects human voice to robot action.
    """
    
    def __init__(self):
        super().__init__('whisper_integration_node')
        
        # Publisher for recognized commands
        self.recognition_publisher = self.create_publisher(String, '/voice/recognized_text', 10)
        self.command_publisher = self.create_publisher(String, '/robot/command', 10)
        
        # Subscribers for audio input
        self.audio_subscriber = self.create_subscription(
            AudioData,
            '/microphone/audio_raw',
            self.audio_callback,
            10
        )
        
        # Initialize Whisper model
        self.get_logger().info('Loading Whisper model...')
        try:
            # Load a smaller model for real-time applications
            self.model = whisper.load_model("base.en")
            self.get_logger().info('Whisper model loaded successfully')
        except Exception as e:
            self.get_logger().error(f'Failed to load Whisper model: {e}')
            self.model = None
            
        # Audio buffer for streaming processing
        self.audio_buffer = deque(maxlen=48000)  # 3 seconds at 16kHz (16-bit samples)
        
        # Process audio in a separate thread
        self.processing_active = True
        self.processing_thread = threading.Thread(target=self.process_audio_continuously)
        self.processing_thread.start()
        
        # Lock for thread-safe buffer access
        self.buffer_lock = threading.Lock()
        
        self.get_logger().info('Whisper integration node initialized')
        
    def audio_callback(self, msg):
        """Handle incoming audio data"""
        if self.model is None:
            return
            
        # Convert 16-bit audio data to float
        audio_int16 = np.frombuffer(msg.data, dtype=np.int16)
        audio_float32 = audio_int16.astype(np.float32) / 32768.0
        
        # Add to buffer (thread-safe)
        with self.buffer_lock:
            for sample in audio_float32:
                self.audio_buffer.append(sample)
                
    def process_audio_continuously(self):
        """Continuously process audio for voice commands"""
        while self.processing_active and rclpy.ok():
            time.sleep(0.5)  # Process every 0.5 seconds
            
            with self.buffer_lock:
                if len(self.audio_buffer) >= 16000:  # At least 1 second of audio
                    # Convert buffer to numpy array
                    audio_array = np.array(list(self.audio_buffer))
                    
                    # Process if we have at least 1 second of audio
                    if len(audio_array) >= 16000:
                        # Take the last 4 seconds of audio for context
                        audio_segment = audio_array[-64000:] if len(audio_array) > 64000 else audio_array
                        
                        # Process with Whisper
                        self.process_audio_segment(audio_segment)
        
    def process_audio_segment(self, audio_segment):
        """Process a segment of audio with Whisper"""
        try:
            # Run Whisper transcription
            result = self.model.transcribe(audio_segment, language='en')
            recognized_text = result['text'].strip()
            
            if recognized_text and len(recognized_text) > 3:  # Filter out short/empty results
                self.get_logger().info(f'Recognized: "{recognized_text}"')
                
                # Publish recognized text
                text_msg = String()
                text_msg.data = recognized_text
                self.recognition_publisher.publish(text_msg)
                
                # Try to extract a command from the recognized text
                command = self.extract_command(recognized_text)
                if command:
                    cmd_msg = String()
                    cmd_msg.data = command
                    self.command_publisher.publish(cmd_msg)
                    self.get_logger().info(f'Sent command: {command}')
                    
        except Exception as e:
            self.get_logger().error(f'Error in Whisper processing: {e}')
            
    def extract_command(self, text: str) -> Optional[str]:
        """Extract robot command from recognized text"""
        text_lower = text.lower()
        
        # Simple command extraction - in a real system this would be more sophisticated
        if 'move forward' in text_lower or 'go forward' in text_lower:
            return 'move_forward'
        elif 'move backward' in text_lower or 'go back' in text_lower:
            return 'move_backward'
        elif 'turn left' in text_lower:
            return 'turn_left'
        elif 'turn right' in text_lower:
            return 'turn_right'
        elif 'stop' in text_lower:
            return 'stop'
        elif 'pick up' in text_lower or 'grasp' in text_lower:
            return 'grasp_object'
        elif 'put down' in text_lower or 'place' in text_lower:
            return 'place_object'
        # Add more commands as needed
            
        return None
        
    def destroy_node(self):
        """Clean up resources when node is destroyed"""
        self.processing_active = False
        if self.processing_thread.is_alive():
            self.processing_thread.join(timeout=1.0)
        super().destroy_node()

def main(args=None):
    rclpy.init(args=args)
    whisper_integration_node = WhisperIntegrationNode()
    
    try:
        rclpy.spin(whisper_integration_node)
    except KeyboardInterrupt:
        pass
    finally:
        whisper_integration_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Hands-On Example

In this hands-on example, we'll implement a complete Whisper voice command system:

1. **Setup Whisper Environment**: Install and configure Whisper for robotic use
2. **Implement Audio Processing**: Create audio capture and preprocessing pipeline
3. **Integrate Speech Recognition**: Connect Whisper to ROS 2 communication
4. **Test Voice Commands**: Validate recognition and command execution
5. **Deploy to Robot**: Integrate with actual robot hardware

### Step 1: Create Whisper configuration file (whisper_config.yaml)
```yaml
# Whisper Configuration for Voice Command Processing
whisper_voice_commands:
  model:
    name: "base.en"  # Options: tiny.en, base.en, small.en, medium.en, large
    device: "cuda"  # Use "cuda" for GPU acceleration if available, "cpu" otherwise
    compute_type: "float16"  # Options: float16, float32
    
  audio_processing:
    sample_rate: 16000  # Hz
    channels: 1  # Mono
    bit_depth: 16  # bits per sample
    buffer_duration: 4.0  # seconds to buffer before processing
    silence_threshold: -40  # dB, below which audio is considered silent
    min_speech_duration: 0.5  # seconds, minimum duration of speech to process
    
  recognition:
    language: "en"  # Language code (en, es, fr, etc.)
    temperature: [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]  # List of temperatures to try
    compression_ratio_threshold: 2.4  # For filtering out low-quality results
    logprob_threshold: -1.0  # For filtering out low-quality results
    no_speech_threshold: 0.6  # For detecting non-speech segments
    
  commands:
    movement:
      - "move forward"
      - "go forward" 
      - "move back"
      - "go back"
      - "move backward"
      - "go backward"
      - "turn left"
      - "turn right"
      - "rotate left"
      - "rotate right"
      - "stop"
    manipulation:
      - "pick up"
      - "pick up the object"
      - "grasp"
      - "grasp the object"
      - "put down"
      - "place"
      - "place the object"
      - "release"
    navigation:
      - "go to kitchen"
      - "go to living room"
      - "go to bedroom"
      - "go to office"
      - "come to me"
      - "come here"
    
  performance:
    max_processing_time: 2.0  # seconds to process audio before timeout
    target_latency: 1.0  # seconds from audio input to command output
    minimum_confidence: 0.7  # minimum confidence for accepting recognition
    
  safety:
    emergency_keywords:
      - "stop immediately"
      - "emergency stop"
      - "halt"
      - "cease"
    safety_command_publisher: "/safety/stop"
    
  audio_input:
    device_index: 0  # Index of microphone device to use
    chunk_size: 1024  # samples per audio chunk
    input_timeout: 5.0  # seconds to wait for audio input
    
  debug:
    log_recognitions: true
    publish_audio_segments: false
    publish_intermediate_results: false
    statistics_collection: true
```

Each step connects to the simulation-to-reality learning pathway.

## Real-World Application

<Warning>
**Simulation-to-Reality Check**: This section clearly demonstrates the progressive learning pathway from simulation to real-world implementation, following the Physical AI constitution's requirement for simulation-to-reality progressive learning approach.
</Warning>

In real-world robotics applications, Whisper voice commands are valuable for:

- Natural human-robot interaction in domestic and commercial environments
- Hands-free operation for users with mobility limitations
- Flexible task specification using natural language
- Accessibility enhancement for diverse user populations

When transitioning from simulation to reality, Whisper voice command systems must account for:

- Real acoustic environments with noise and reverberation
- Variability in human speaking patterns and accents
- Computational constraints of robot hardware
- Privacy considerations for speech processing

The Whisper integration enables the Physical AI principle of simulation-to-reality progressive learning by providing robots with the capability to understand spoken language commands, connecting computational speech recognition to physical robot embodiment and action execution.

## Summary

This chapter covered the fundamentals of using OpenAI Whisper for voice commands:
- How Whisper performs speech recognition for robotic applications
- Core components of Whisper-based voice command systems
- Technical implementation of audio processing and command recognition
- Practical example of Whisper integration with robotic systems
- Real-world considerations for deploying on physical hardware

Whisper provides robots with the capability to understand spoken language commands, enabling effective embodied intelligence applications, supporting the Physical AI principle of connecting computational speech recognition to physical robot embodiment.

## Key Terms

<dl>
<dt>Automatic Speech Recognition (ASR)</dt>
<dd>The technology that converts spoken language into text in the Physical AI context.</dd>

<dt>OpenAI Whisper</dt>
<dd>A transformer-based speech recognition model developed by OpenAI for converting speech to text.</dd>

<dt>Voice Command Processing</dt>
<dd>The complete pipeline from speech input to robot action execution.</dd>

<dt>Speech-to-Text Pipeline</dt>
<dd>The sequence of processing steps from audio input to textual output.</dd>
</dl>

---

## Compliance Check

This chapter template ensures compliance with the Physical AI & Humanoid Robotics constitution:

- ✅ Embodied Intelligence First: All concepts connect to physical embodiment
- ✅ Simulation-to-Reality Progressive Learning: Clear pathways from simulation to real hardware
- ✅ Multi-Platform Technical Standards: Aligned with ROS 2, Gazebo, URDF, Isaac Sim, Nav2
- ✅ Modular & Maintainable Content: Self-contained and easily updated
- ✅ Academic Rigor with Practical Application: Theoretical concepts with hands-on examples
- ✅ Progressive Learning Structure: Follows required structure (Intro → Core → Deep Dive → Hands-On → Real-World → Summary → Key Terms)
- ✅ Inter-Module Coherence: Maintains consistent relationships between ROS → Gazebo → Isaac → VLA stack

## Inter-Module Coherence

<Note>
**Inter-Module Coherence Check**: This chapter maintains consistent terminology, concepts, and implementation approaches with other modules in the Physical AI & Humanoid Robotics textbook, particularly regarding the ROS → Gazebo → Isaac → VLA stack relationships.

This chapter establishes the voice processing that connects to other modules:
- The voice interface integrates with ROS communication from Module 1
- Voice commands connect with visual perception from Module 2
- Speech recognition enhances Isaac capabilities from Module 3
- Voice processing is essential for VLA integration in Module 4
</Note>